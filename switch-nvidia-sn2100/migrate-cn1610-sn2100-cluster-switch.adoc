---
permalink: switch-nvidia-sn2100/migrate-cn1610-sn2100-cluster-switch.html 
sidebar: sidebar 
keywords: migrate cluster NVIDIA SN2100 cluster switches cn1610 
summary: È possibile migrare senza interruzioni gli switch cluster NetApp CN1610 per un cluster ONTAP verso gli switch di rete cluster NVIDIA SN2100. 
---
= Migrazione degli switch cluster CN1610 agli switch cluster NVIDIA SN2100
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
È possibile migrare gli switch cluster NetApp CN1610 per un cluster ONTAP verso gli switch cluster NVIDIA SN2100. Si tratta di una procedura non distruttiva.



== Requisiti di revisione

Quando si sostituiscono gli switch cluster NetApp CN1610 con gli switch cluster NVIDIA SN2100, è necessario conoscere alcune informazioni sulla configurazione, sulle connessioni delle porte e sui requisiti di cablaggio. Vederelink:configure-overview-sn2100-cluster.html["Panoramica dell'installazione e della configurazione degli switch NVIDIA SN2100"] .

.Switch supportati
Sono supportati i seguenti switch cluster:

* NetApp CN1610
* NVIDIA SN2100


Per i dettagli sulle porte supportate e le relative configurazioni, vedere https://hwu.netapp.com/["Hardware Universe"^] .

.Prima di iniziare
Verifica di soddisfare i seguenti requisiti per la tua configurazione:

* Il cluster esistente è configurato e funzionante correttamente.
* Tutte le porte del cluster sono nello stato *attivo* per garantire operazioni senza interruzioni.
* Gli switch cluster NVIDIA SN2100 sono configurati e funzionano con la versione corretta di Cumulus Linux installata con il file di configurazione di riferimento (RCF) applicato.
* La configurazione di rete del cluster esistente è la seguente:
+
** Un cluster NetApp ridondante e completamente funzionale che utilizza switch CN1610.
** Connettività di gestione e accesso alla console sia per gli switch CN1610 che per i nuovi switch.
** Tutti i cluster LIF sono attivi con i cluster LIf sulle rispettive porte home.
** Porte ISL abilitate e cablate tra gli switch CN1610 e tra i nuovi switch.


* Alcune porte sono configurate sugli switch NVIDIA SN2100 per funzionare a 40GbE o 100GbE.
* Hai pianificato, migrato e documentato la connettività 40GbE e 100GbE dai nodi agli switch cluster NVIDIA SN2100.




== Migrare gli switch

.Informazioni sugli esempi
Gli esempi in questa procedura utilizzano la seguente nomenclatura di switch e nodi:

* Gli switch cluster CN1610 esistenti sono _c1_ e _c2_.
* I nuovi switch cluster NVIDIA SN2100 sono _sw1_ e _sw2_.
* I nodi sono _node1_ e _node2_.
* I LIF del cluster sono _node1_clus1_ e _node1_clus2_ sul nodo 1, e _node2_clus1_ e _node2_clus2_ sul nodo 2, rispettivamente.
* IL `cluster1::*>` il prompt indica il nome del cluster.
* Le porte del cluster utilizzate in questa procedura sono _e3a_ e _e3b_.
* Le porte breakout hanno il formato: swp[porta]s[porta breakout 0-3].  Ad esempio, quattro porte breakout su swp1 sono _swp1s0_, _swp1s1_, _swp1s2_ e _swp1s3_.


.Informazioni su questo compito
Questa procedura copre il seguente scenario:

* Lo switch c2 viene sostituito prima dallo switch sw2.
+
** Chiudere le porte verso i nodi del cluster.  Per evitare l'instabilità del cluster, tutte le porte devono essere chiuse contemporaneamente.
** Il cablaggio tra i nodi e c2 viene quindi scollegato da c2 e ricollegato a sw2.


* L'interruttore c1 è sostituito dall'interruttore sw1.
+
** Chiudere le porte verso i nodi del cluster.  Per evitare l'instabilità del cluster, tutte le porte devono essere chiuse contemporaneamente.
** Il cablaggio tra i nodi e c1 viene quindi scollegato da c1 e ricollegato a sw1.





NOTE: Durante questa procedura non è necessario alcun collegamento inter-switch (ISL) operativo. Ciò è voluto perché le modifiche alla versione RCF possono influire temporaneamente sulla connettività ISL. Per garantire operazioni del cluster senza interruzioni, la seguente procedura migra tutti i LIF del cluster allo switch partner operativo, eseguendo al contempo i passaggi sullo switch di destinazione.



=== Fase 1: Prepararsi alla migrazione

. Se AutoSupport è abilitato su questo cluster, sopprimere la creazione automatica dei casi richiamando un messaggio AutoSupport :
+
`system node autosupport invoke -node * -type all -message MAINT=xh`

+
dove _x_ è la durata della finestra di manutenzione in ore.

. Modificare il livello di privilegio in avanzato, immettendo *y* quando richiesto per continuare:
+
`set -privilege advanced`

+
Viene visualizzato il prompt avanzato (*>).

. Disabilitare il ripristino automatico sui LIF del cluster:
+
`network interface modify -vserver Cluster -lif * -auto-revert false`





=== Passaggio 2: configurare porte e cablaggio

. Determinare lo stato amministrativo o operativo per ciascuna interfaccia del cluster.
+
Ogni porta dovrebbe essere visualizzata per `Link` E `healthy` per `Health Status` .

+
.. Visualizza gli attributi della porta di rete:
+
`network port show -ipspace Cluster`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false

Node: node2
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false
----
====
.. Visualizza informazioni sui LIF e sui nodi domestici designati:
+
`network interface show -vserver Cluster`

+
Ogni LIF dovrebbe visualizzare `up/up` per `Status Admin/Oper` E `true` per `Is Home` .

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*

            Logical      Status     Network            Current     Current Is
Vserver     Interface    Admin/Oper Address/Mask       Node        Port    Home
----------- -----------  ---------- ------------------ ----------- ------- ----
Cluster
            node1_clus1  up/up      169.254.209.69/16  node1       e3a     true
            node1_clus2  up/up      169.254.49.125/16  node1       e3b     true
            node2_clus1  up/up      169.254.47.194/16  node2       e3a     true
            node2_clus2  up/up      169.254.19.183/16  node2       e3b     true

----
====


. Le porte del cluster su ciascun nodo vengono collegate agli switch del cluster esistenti nel modo seguente (dal punto di vista dei nodi) utilizzando il comando:
+
`network device-discovery show -protocol`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node1      /cdp
            e3a    c1 (6a:ad:4f:98:3b:3f)    0/1               -
            e3b    c2 (6a:ad:4f:98:4c:a4)    0/1               -
node2      /cdp
            e3a    c1 (6a:ad:4f:98:3b:3f)    0/2               -
            e3b    c2 (6a:ad:4f:98:4c:a4)    0/2               -
----
====
. Le porte e gli switch del cluster vengono collegati nel modo seguente (dal punto di vista degli switch) utilizzando il comando:
+
`show cdp neighbors`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
c1# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID             Local Intrfce Hldtme Capability  Platform         Port ID
node1                 0/1           124     H          AFF-A400         e3a
node2                 0/2           124     H          AFF-A400         e3a
c2                    0/13          179     S I s      CN1610           0/13
c2                    0/14          175     S I s      CN1610           0/14
c2                    0/15          179     S I s      CN1610           0/15
c2                    0/16          175     S I s      CN1610           0/16

c2# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute


Device-ID             Local Intrfce Hldtme Capability  Platform         Port ID
node1                 0/1           124    H           AFF-A400         e3b
node2                 0/2           124    H           AFF-A400         e3b
c1                    0/13          175    S I s       CN1610           0/13
c1                    0/14          175    S I s       CN1610           0/14
c1                    0/15          175    S I s       CN1610           0/15
c1                    0/16          175    S I s       CN1610           0/16
----
====
. Verificare la connettività delle interfacce del cluster remoto:


[role="tabbed-block"]
====
.ONTAP 9.9.1 e versioni successive
--
Puoi usare il `network interface check cluster-connectivity` comando per avviare un controllo di accessibilità per la connettività del cluster e quindi visualizzare i dettagli:

`network interface check cluster-connectivity start`E `network interface check cluster-connectivity show`

[listing, subs="+quotes"]
----
cluster1::*> *network interface check cluster-connectivity start*
----
*NOTA:* Attendere alcuni secondi prima di eseguire il `show` comando per visualizzare i dettagli.

[listing, subs="+quotes"]
----
cluster1::*> *network interface check cluster-connectivity show*
                                  Source           Destination      Packet
Node   Date                       LIF              LIF              Loss
------ -------------------------- ---------------- ---------------- -----------
node1
       3/5/2022 19:21:18 -06:00   node1_clus2      node2-clus1      none
       3/5/2022 19:21:20 -06:00   node1_clus2      node2_clus2      none
node2
       3/5/2022 19:21:18 -06:00   node2_clus2      node1_clus1      none
       3/5/2022 19:21:20 -06:00   node2_clus2      node1_clus2      none
----
--
.Tutte le versioni ONTAP
--
Per tutte le versioni ONTAP , è anche possibile utilizzare `cluster ping-cluster -node <name>` comando per verificare la connettività:

`cluster ping-cluster -node <name>`

[listing, subs="+quotes"]
----
cluster1::*> *cluster ping-cluster -node local*
Host is node2
Getting addresses from network interface table...
Cluster node1_clus1 169.254.209.69 node1     e3a
Cluster node1_clus2 169.254.49.125 node1     e3b
Cluster node2_clus1 169.254.47.194 node2     e3a
Cluster node2_clus2 169.254.19.183 node2     e3b
Local = 169.254.47.194 169.254.19.183
Remote = 169.254.209.69 169.254.49.125
Cluster Vserver Id = 4294967293
Ping status:....
Basic connectivity succeeds on 4 path(s)
Basic connectivity fails on 0 path(s)
................
Detected 9000 byte MTU on 4 path(s):
    Local 169.254.19.183 to Remote 169.254.209.69
    Local 169.254.19.183 to Remote 169.254.49.125
    Local 169.254.47.194 to Remote 169.254.209.69
    Local 169.254.47.194 to Remote 169.254.49.125
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
----
--
====
. [[step5]]Sullo switch c2, chiudere le porte connesse alle porte del cluster dei nodi per eseguire il failover dei LIF del cluster.
+
[listing, subs="+quotes"]
----
(c2)# *configure*
(c2)(Config)# *interface 0/1-0/12*
(c2)(Interface 0/1-0/12)# *shutdown*
(c2)(Interface 0/1-0/12)# *exit*
(c2)(Config)# *exit*
(c2)#
----
. Spostare le porte del cluster di nodi dal vecchio switch c2 al nuovo switch sw2, utilizzando un cablaggio appropriato supportato da NVIDIA SN2100.
. Visualizza gli attributi della porta di rete:
+
`network port show -ipspace Cluster`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false

Node: node2
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false
----
====
. Le porte del cluster su ciascun nodo sono ora collegate agli switch del cluster nel modo seguente, dal punto di vista dei nodi:
+
`network device-discovery show -protocol`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol lldp*

Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node1      /lldp
            e3a    c1  (6a:ad:4f:98:3b:3f)   0/1               -
            e3b    sw2 (b8:ce:f6:19:1a:7e)   swp3              -
node2      /lldp
            e3a    c1  (6a:ad:4f:98:3b:3f)   0/2               -
            e3b    sw2 (b8:ce:f6:19:1b:96)   swp4              -
----
====
. Sullo switch sw2, verificare che tutte le porte del cluster dei nodi siano attive:
+
`net show interface`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@sw2:~$ *net show interface*

State  Name         Spd   MTU    Mode        LLDP              Summary
-----  -----------  ----  -----  ----------  ----------------- ----------------------
...
...
UP     swp3         100G  9216   Trunk/L2    e3b               Master: bridge(UP)
UP     swp4         100G  9216   Trunk/L2    e3b               Master: bridge(UP)
UP     swp15        100G  9216   BondMember  sw1 (swp15)       Master: cluster_isl(UP)
UP     swp16        100G  9216   BondMember  sw1 (swp16)       Master: cluster_isl(UP)
----
====
. Sullo switch c1, chiudere le porte collegate alle porte del cluster dei nodi per eseguire il failover dei LIF del cluster.
+
[listing, subs="+quotes"]
----
(c1)# *configure*
(c1)(Config)# *interface 0/1-0/12*
(c1)(Interface 0/1-0/12)# *shutdown*
(c1)(Interface 0/1-0/12)# *exit*
(c1)(Config)# *exit*
(c1)#
----
. Spostare le porte del cluster di nodi dal vecchio switch c1 al nuovo switch sw1, utilizzando un cablaggio appropriato supportato da NVIDIA SN2100.
. Verificare la configurazione finale del cluster:
+
`network port show -ipspace Cluster`

+
Ogni porta dovrebbe visualizzare `up` per `Link` E `healthy` per `Health Status` .

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false

Node: node2
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false
----
====
. Le porte del cluster su ciascun nodo sono ora collegate agli switch del cluster nel modo seguente, dal punto di vista dei nodi:
+
`network device-discovery show -protocol`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol lldp*

Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface       Platform
----------- ------ ------------------------- --------------  ----------------
node1      /lldp
            e3a    sw1 (b8:ce:f6:19:1a:7e)   swp3            -
            e3b    sw2 (b8:ce:f6:19:1b:96)   swp3            -
node2      /lldp
            e3a    sw1 (b8:ce:f6:19:1a:7e)   swp4            -
            e3b    sw2 (b8:ce:f6:19:1b:96)   swp4            -
----
====
. Sugli switch sw1 e sw2, verificare che tutte le porte del cluster di nodi siano attive:
+
`net show interface`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net show interface*

State  Name         Spd   MTU    Mode        LLDP              Summary
-----  -----------  ----  -----  ----------  ----------------- ----------------------
...
...
UP     swp3         100G  9216   Trunk/L2    e3a               Master: bridge(UP)
UP     swp4         100G  9216   Trunk/L2    e3a               Master: bridge(UP)
UP     swp15        100G  9216   BondMember  sw2 (swp15)       Master: cluster_isl(UP)
UP     swp16        100G  9216   BondMember  sw2 (swp16)       Master: cluster_isl(UP)


cumulus@sw2:~$ *net show interface*

State  Name         Spd   MTU    Mode        LLDP              Summary
-----  -----------  ----  -----  ----------  ----------------- -----------------------
...
...
UP     swp3         100G  9216   Trunk/L2    e3b               Master: bridge(UP)
UP     swp4         100G  9216   Trunk/L2    e3b               Master: bridge(UP)
UP     swp15        100G  9216   BondMember  sw1 (swp15)       Master: cluster_isl(UP)
UP     swp16        100G  9216   BondMember  sw1 (swp16)       Master: cluster_isl(UP)
----
====
. Verificare che entrambi i nodi abbiano una connessione a ciascun switch:
+
`net show lldp`

+
.Mostra esempio
[%collapsible]
====
L'esempio seguente mostra i risultati appropriati per entrambi gli switch:

[listing, subs="+quotes"]
----
cumulus@sw1:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost          RemotePort
---------  -----  ----------  ------------------  -----------
swp3       100G   Trunk/L2    node1               e3a
swp4       100G   Trunk/L2    node2               e3a
swp15      100G   BondMember  sw2                 swp15
swp16      100G   BondMember  sw2                 swp16

cumulus@sw2:~$ *net show lldp*

LocalPort  Speed  Mode        RemoteHost          RemotePort
---------  -----  ----------  ------------------  -----------
swp3       100G   Trunk/L2    node1               e3b
swp4       100G   Trunk/L2    node2               e3b
swp15      100G   BondMember  sw1                 swp15
swp16      100G   BondMember  sw1                 swp16
----
====




=== Passaggio 3: verificare la configurazione

. Abilita il ripristino automatico sui LIF del cluster:
+
`cluster1::*> network interface modify -vserver Cluster -lif * -auto-revert true`

. Sullo switch sw2, arrestare e riavviare tutte le porte del cluster per attivare un ripristino automatico di tutti i LIF del cluster che non si trovano sulle loro porte home.


[role="tabbed-block"]
====
.Cumulus 4.4.3
--
[listing, subs="+quotes"]
----
cumulus@sw2:mgmt:~$ *net add interface swp1-14 link down*
cumulus@sw2:mgmt:~$ *net pending*
cumulus@sw2:mgmt:~$ *net commit*

(Wait for 5-10 seconds before re-enabling the ports)

cumulus@sw2:mgmt:~$ *net add interface swp1-14 link up*
cumulus@sw2:mgmt:~$ *net pending*
cumulus@sw2:mgmt:~$ *net commit*

(After executing the link state up command, the nodes detect the change and begin to auto-revert the cluster LIFs to their home ports)
----
--
.Cumulus 5.x
--
[listing, subs="+quotes"]
----
cumulus@sw2:mgmt:~$ *nv set interface swp1-14 link state down*
cumulus@sw2:mgmt:~$ *nv config apply*
cumulus@sw2:mgmt:~$ *nv show interface*

(Wait for 5-10 seconds before re-enabling the ports)

cumulus@sw2:mgmt:~$ *nv set interface swp1-14 link state up*
cumulus@sw2:mgmt:~$ *nv config apply*
cumulus@sw2:mgmt:~$ *nv show interface*

(After executing the link state up command, the nodes detect the change and begin to auto-revert the cluster LIFs to their home ports)
----
--
====
. [[step3]]Verificare che i LIF del cluster siano tornati alle loro porte home (potrebbe volerci un minuto):
+
`network interface show -vserver Cluster`

+
Se uno qualsiasi dei LIF del cluster non è tornato alla propria porta home, ripristinarlo manualmente. È necessario connettersi a ciascuna console di sistema LIF o SP/ BMC di gestione nodi del nodo locale proprietario del LIF:

+
`network interface revert -vserver Cluster -lif *`

. Ripristinare il livello di privilegio su amministratore:
+
`set -privilege admin`

. Se hai disattivato la creazione automatica dei casi, riattivala richiamando un messaggio AutoSupport :
+
`system node autosupport invoke -node * -type all -message MAINT=END`



.Cosa succederà ora?
Dopo aver migrato gli switch, puoi link:../switch-cshm/config-overview.html["configurare il monitoraggio dello stato dello switch"].
