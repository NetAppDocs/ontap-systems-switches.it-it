---
permalink: switch-cisco-92300/migrate-to-92300yc.html 
sidebar: sidebar 
keywords: migration steps for cisco nexus 92300yc cluster switch 
summary: È possibile migrare senza interruzioni gli switch cluster Cisco meno recenti per un cluster ONTAP agli switch di rete del cluster Cisco Nexus 92300YC. 
---
= Migrare da uno switch Cisco a uno switch Cisco Nexus 92300YC
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
È possibile migrare senza interruzioni gli switch cluster Cisco meno recenti per un cluster ONTAP agli switch di rete del cluster Cisco Nexus 92300YC.


NOTE: Una volta completata la migrazione, potrebbe essere necessario installare il file di configurazione richiesto per supportare Cluster Switch Health Monitor (CSHM) per gli switch cluster 92300YC. Vedere link:../switch-cshm/cshm-overview.html["Monitoraggio dello stato di salute degli switch (CSHM)"] .



== Verifica dei requisiti

.Prima di iniziare
Assicurarsi di disporre di quanto segue:

* Un cluster esistente completamente funzionale.
* Connettività 10 GbE e 40 GbE dai nodi agli switch di cluster Nexus 92300YC.
* Tutte le porte del cluster sono in stato attivo per garantire operazioni senza interruzioni.
* Versione corretta di NX-OS e file di configurazione di riferimento (RCF) installati sugli switch cluster Nexus 92300YC.
* Un cluster NetApp ridondante e completamente funzionale che utilizza entrambi gli switch Cisco meno recenti.
* Connettività di gestione e accesso alla console sia agli switch Cisco meno recenti che ai nuovi switch.
* Tutte le LIF del cluster in stato up con le LIF del cluster si trovano sulle porte home.
* Porte ISL abilitate e cablate tra i vecchi switch Cisco e tra i nuovi switch.




== Migrare lo switch

.A proposito degli esempi
Gli esempi di questa procedura utilizzano la seguente nomenclatura di switch e nodi:

* Gli switch cluster Cisco Nexus 5596UP esistenti sono c1 e c2.
* I nuovi switch in cluster Nexus 92300YC sono cs1 e cs2.
* I nodi sono node1 e node2.
* I LIF del cluster sono rispettivamente node1_clus1 e node1_clus2 sul nodo 1, e node2_clus1 e node2_clus2 sul nodo 2.
* Lo switch c2 viene sostituito prima dallo switch cs2, quindi lo switch c1 viene sostituito dallo switch cs1.
+
** Un ISL temporaneo è costruito su cs1 che collega c1 a cs1.
** Il cablaggio tra i nodi e c2 viene quindi scollegato da c2 e ricollegato a cs2.
** Il cablaggio tra i nodi e c1 viene quindi scollegato da c1 e ricollegato a cs1.
** L'ISL temporaneo tra c1 e cs1 viene quindi rimosso.




.Porte utilizzate per le connessioni
* Alcune porte sono configurate su switch Nexus 92300YC per funzionare a 10 GbE o 40 GbE.
* Gli switch del cluster utilizzano le seguenti porte per le connessioni ai nodi:
+
** Porte e1/1-48 (10/25 GbE), e1/49-64 (40/100 GbE): Nexus 92300YC
** Porte e1/1-40 (10 GbE): Nexus 5596UP
** Porte e1/1-32 (10 GbE): Nexus 5020
** Porte e1/1-12, e2/1-6 (10 GbE): Nexus 5010 con modulo di espansione


* Gli switch del cluster utilizzano le seguenti porte ISL (Inter-Switch link):
+
** Porte e1/65-66 (100 GbE): Nexus 92300YC
** Porte e1/41-48 (10 GbE): Nexus 5596UP
** Porte e1/33-40 (10 GbE): Nexus 5020
** Porte e1/13-20 (10 GbE): Nexus 5010


* https://hwu.netapp.com/SWITCH/INDEX["Hardware Universe - Switch"^] contiene informazioni sul cablaggio supportato per tutti gli switch del cluster.
* Le versioni di ONTAP e NX-OS supportate in questa procedura sono disponibili in https://support.netapp.com/NOW/download/software/cm_switches/["Switch Ethernet Cisco"^] pagina.




=== Fase 1: Preparazione per la migrazione

. Impostare il livello di privilegio su Advanced (avanzato), immettendo *y* quando viene richiesto di continuare:
+
`set -privilege advanced`

+
Viene visualizzato il prompt Advanced (*>).

. Se AutoSupport è attivato su questo cluster, eliminare la creazione automatica del caso richiamando un messaggio AutoSupport:
+
`system node autosupport invoke -node * -type all -message MAINT=xh`

+
dove x è la durata della finestra di manutenzione in ore.

+

NOTE: Il messaggio AutoSupport informa il supporto tecnico di questa attività di manutenzione in modo che la creazione automatica del caso venga soppressa durante la finestra di manutenzione.

+
.Mostra esempio
[%collapsible]
====
Il seguente comando elimina la creazione automatica del caso per due ore:

[listing, subs="+quotes"]
----
cluster1::*> *system node autosupport invoke -node * -type all -message MAINT=2h*
----
====
. Verificare che l'autorevert sia attivato su tutte le LIF del cluster:
+
`network interface show -vserver Cluster -fields auto-revert`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster -fields auto-revert*

          Logical
Vserver   Interface     Auto-revert
--------- ------------- ------------
Cluster
          node1_clus1   true
          node1_clus2   true
          node2_clus1   true
          node2_clus2   true

4 entries were displayed.
----
====
. Determinare lo stato amministrativo o operativo di ciascuna interfaccia del cluster:
+
Ogni porta deve essere visualizzata per `Link` e sano per `Health Status`.

+
.. Visualizzare gli attributi della porta di rete:
+
`network port show -ipspace Cluster`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

Node: node2
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

4 entries were displayed.
----
====
.. Visualizzare le informazioni sulle interfacce logiche e sui relativi nodi principali designati:
+
`network interface show -vserver Cluster`

+
Ogni LIF deve visualizzare UP/UP per `Status Admin/Oper` e vero per `Is Home`.

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*

            Logical      Status     Network            Current       Current Is
Vserver     Interface    Admin/Oper Address/Mask       Node          Port    Home
----------- -----------  ---------- ------------------ ------------- ------- ----
Cluster
            node1_clus1  up/up      169.254.209.69/16  node1         e0a     true
            node1_clus2  up/up      169.254.49.125/16  node1         e0b     true
            node2_clus1  up/up      169.254.47.194/16  node2         e0a     true
            node2_clus2  up/up      169.254.19.183/16  node2         e0b     true

4 entries were displayed.
----
====


. Verificare che le porte del cluster su ciascun nodo siano collegate agli switch del cluster esistenti nel seguente modo (dal punto di vista dei nodi) utilizzando il comando:
+
`network device-discovery show -protocol cdp`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node2      /cdp
            e0a    c1                        0/2               N5K-C5596UP
            e0b    c2                        0/2               N5K-C5596UP
node1      /cdp
            e0a    c1                        0/1               N5K-C5596UP
            e0b    c2                        0/1               N5K-C5596UP

4 entries were displayed.
----
====
. Verificare che le porte del cluster e gli switch siano collegati nel modo seguente (dal punto di vista degli switch) utilizzando il comando:
+
`show cdp neighbors`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
c1# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute


Device-ID             Local Intrfce Hldtme Capability  Platform         Port ID
node1               Eth1/1         124    H         FAS2750            e0a
node2               Eth1/2         124    H         FAS2750            e0a
c2(FOX2025GEFC)     Eth1/41        179    S I s     N5K-C5596UP        Eth1/41

c2(FOX2025GEFC)     Eth1/42        175    S I s     N5K-C5596UP        Eth1/42

c2(FOX2025GEFC)     Eth1/43        179    S I s     N5K-C5596UP        Eth1/43

c2(FOX2025GEFC)     Eth1/44        175    S I s     N5K-C5596UP        Eth1/44

c2(FOX2025GEFC)     Eth1/45        179    S I s     N5K-C5596UP        Eth1/45

c2(FOX2025GEFC)     Eth1/46        179    S I s     N5K-C5596UP        Eth1/46

c2(FOX2025GEFC)     Eth1/47        175    S I s     N5K-C5596UP        Eth1/47

c2(FOX2025GEFC)     Eth1/48        179    S I s     N5K-C5596UP        Eth1/48

Total entries displayed: 10


c2# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute


Device-ID             Local Intrfce Hldtme Capability  Platform         Port ID
node1               Eth1/1         124    H         FAS2750            e0b
node2               Eth1/2         124    H         FAS2750            e0b
c1(FOX2025GEEX)     Eth1/41        175    S I s     N5K-C5596UP        Eth1/41

c1(FOX2025GEEX)     Eth1/42        175    S I s     N5K-C5596UP        Eth1/42

c1(FOX2025GEEX)     Eth1/43        175    S I s     N5K-C5596UP        Eth1/43

c1(FOX2025GEEX)     Eth1/44        175    S I s     N5K-C5596UP        Eth1/44

c1(FOX2025GEEX)     Eth1/45        175    S I s     N5K-C5596UP        Eth1/45

c1(FOX2025GEEX)     Eth1/46        175    S I s     N5K-C5596UP        Eth1/46

c1(FOX2025GEEX)     Eth1/47        176    S I s     N5K-C5596UP        Eth1/47

c1(FOX2025GEEX)     Eth1/48        176    S I s     N5K-C5596UP        Eth1/48
----
====
. Verificare la connettività delle interfacce del cluster remoto:


[role="tabbed-block"]
====
.ONTAP 9.9.1 e versioni successive
--
È possibile utilizzare `network interface check cluster-connectivity` per avviare un controllo di accessibilità per la connettività del cluster e visualizzare i dettagli:

`network interface check cluster-connectivity start` e. `network interface check cluster-connectivity show`

[listing, subs="+quotes"]
----
cluster1::*> *network interface check cluster-connectivity start*
----
*NOTA:* attendere alcuni secondi prima di eseguire il `show` comando per visualizzare i dettagli.

[listing, subs="+quotes"]
----
cluster1::*> *network interface check cluster-connectivity show*
                                  Source           Destination      Packet
Node   Date                       LIF              LIF              Loss
------ -------------------------- ---------------- ---------------- -----------
node1
       3/5/2022 19:21:18 -06:00   node1_clus2      node2-clus1      none
       3/5/2022 19:21:20 -06:00   node1_clus2      node2_clus2      none
node2
       3/5/2022 19:21:18 -06:00   node2_clus2      node1_clus1      none
       3/5/2022 19:21:20 -06:00   node2_clus2      node1_clus2      none
----
--
.Tutte le release di ONTAP
--
Per tutte le release di ONTAP, è possibile utilizzare anche `cluster ping-cluster -node <name>` comando per controllare la connettività:

`cluster ping-cluster -node <name>`

[listing, subs="+quotes"]
----
cluster1::*> *cluster ping-cluster -node local*
Host is node2
Getting addresses from network interface table...
Cluster node1_clus1 169.254.209.69 node1     e0a
Cluster node1_clus2 169.254.49.125 node1     e0b
Cluster node2_clus1 169.254.47.194 node2     e0a
Cluster node2_clus2 169.254.19.183 node2     e0b
Local = 169.254.47.194 169.254.19.183
Remote = 169.254.209.69 169.254.49.125
Cluster Vserver Id = 4294967293
Ping status:
....
Basic connectivity succeeds on 4 path(s)
Basic connectivity fails on 0 path(s)
................
Detected 9000 byte MTU on 4 path(s):
    Local 169.254.19.183 to Remote 169.254.209.69
    Local 169.254.19.183 to Remote 169.254.49.125
    Local 169.254.47.194 to Remote 169.254.209.69
    Local 169.254.47.194 to Remote 169.254.49.125
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
----
--
====


=== Fase 2: Configurazione di cavi e porte

. Configurare un ISL temporaneo su cs1sulle porte e1/41-48, tra c1 e cs1.
+
.Mostra esempio
[%collapsible]
====
L'esempio seguente mostra come il nuovo ISL viene configurato su c1 e cs1:

[listing, subs="+quotes"]
----
cs1# *configure*
Enter configuration commands, one per line. End with CNTL/Z.
cs1(config)# *interface e1/41-48*
cs1(config-if-range)# *description temporary ISL between Nexus 5596UP and Nexus 92300YC*
cs1(config-if-range)# *no lldp transmit*
cs1(config-if-range)# *no lldp receive*
cs1(config-if-range)# *switchport mode trunk*
cs1(config-if-range)# *no spanning-tree bpduguard enable*
cs1(config-if-range)# *channel-group 101 mode active*
cs1(config-if-range)# *exit*
cs1(config)# *interface port-channel 101*
cs1(config-if)# *switchport mode trunk*
cs1(config-if)# *spanning-tree port type network*
cs1(config-if)# *exit*
cs1(config)# *exit*
----
====
. Rimuovere i cavi ISL dalle porte e1/41-48 da c2 e collegarli alle porte e1/41-48 su cs1.
. Verificare che le porte ISL e il port-channel siano operativi collegando c1 e cs1:
+
`show port-channel summary`

+
.Mostra esempio
[%collapsible]
====
Nell'esempio seguente viene illustrato il comando Cisco show port-channel summary utilizzato per verificare che le porte ISL siano operative su c1 e cs1:

[listing, subs="+quotes"]
----
c1# *show port-channel summary*
Flags:  D - Down        P - Up in port-channel (members)
        I - Individual  H - Hot-standby (LACP only)
        s - Suspended   r - Module-removed
        b - BFD Session Wait
        S - Switched    R - Routed
        U - Up (port-channel)
        p - Up in delay-lacp mode (member)
        M - Not in use. Min-links not met
--------------------------------------------------------------------------------
Group Port-       Type     Protocol  Member Ports
      Channel
--------------------------------------------------------------------------------
1     Po1(SU)     Eth      LACP      Eth1/41(P)   Eth1/42(P)   Eth1/43(P)
                                     Eth1/44(P)   Eth1/45(P)   Eth1/46(P)
                                     Eth1/47(P)   Eth1/48(P)


cs1# *show port-channel summary*
Flags:  D - Down        P - Up in port-channel (members)
        I - Individual  H - Hot-standby (LACP only)
        s - Suspended   r - Module-removed
        b - BFD Session Wait
        S - Switched    R - Routed
        U - Up (port-channel)
        p - Up in delay-lacp mode (member)
        M - Not in use. Min-links not met
--------------------------------------------------------------------------------
Group Port-       Type     Protocol  Member Ports
      Channel
--------------------------------------------------------------------------------
1     Po1(SU)     Eth      LACP      Eth1/65(P)   Eth1/66(P)
101   Po101(SU)   Eth      LACP      Eth1/41(P)   Eth1/42(P)   Eth1/43(P)
                                     Eth1/44(P)   Eth1/45(P)   Eth1/46(P)
                                     Eth1/47(P)   Eth1/48(P)
----
====
. Per il nodo 1, scollegare il cavo da e1/1 su c2, quindi collegarlo a e1/1 su cs2, utilizzando il cablaggio appropriato supportato da Nexus 92300YC.
. Per il nodo 2, scollegare il cavo da e1/2 su c2, quindi collegarlo a e1/2 su cs2, utilizzando il cablaggio appropriato supportato da Nexus 92300YC.
. Le porte del cluster su ciascun nodo sono ora collegate agli switch del cluster nel seguente modo, dal punto di vista dei nodi:
+
`network device-discovery show -protocol cdp`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol cdp*

Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node2      /cdp
            e0a    c1                        0/2               N5K-C5596UP
            e0b    cs2                       0/2               N9K-C92300YC
node1      /cdp
            e0a    c1                        0/1               N5K-C5596UP
            e0b    cs2                       0/1               N9K-C92300YC

4 entries were displayed.
----
====
. Per il nodo 1, scollegare il cavo da e1/1 su c1, quindi collegarlo a e1/1 su cs1, utilizzando il cablaggio appropriato supportato da Nexus 92300YC.
. Per il nodo 2, scollegare il cavo da e1/2 su c1, quindi collegarlo a e1/2 su cs1, utilizzando il cablaggio appropriato supportato da Nexus 92300YC.
. Le porte del cluster su ciascun nodo sono ora collegate agli switch del cluster nel seguente modo, dal punto di vista dei nodi:
+
`network device-discovery show -protocol cdp`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node2      /cdp
            e0a    cs1                       0/2               N9K-C92300YC
            e0b    cs2                       0/2               N9K-C92300YC
node1      /cdp
            e0a    cs1                       0/1               N9K-C92300YC
            e0b    cs2                       0/1               N9K-C92300YC
4 entries were displayed.
----
====
. Eliminare l'ISL temporaneo tra cs1 e c1.
+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cs1(config)# *no interface port-channel 10*
cs1(config)# *interface e1/41-48*
cs1(config-if-range)# *lldp transmit*
cs1(config-if-range)# *lldp receive*
cs1(config-if-range)# *no switchport mode trunk*
cs1(config-if-range)# *no channel-group*
cs1(config-if-range)# *description 10GbE Node Port*
cs1(config-if-range)# *spanning-tree bpduguard enable*
cs1(config-if-range)# *exit*
cs1(config)# *exit*
----
====




=== Fase 3: Completare la migrazione

. Verificare la configurazione finale del cluster:
+
`network port show -ipspace Cluster`

+
Ogni porta deve essere visualizzata per `Link` e sano per `Health Status`.

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

Node: node2
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

4 entries were displayed.


cluster1::*> *network interface show -vserver Cluster*

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
Cluster
            node1_clus1  up/up    169.254.209.69/16  node1         e0a     true
            node1_clus2  up/up    169.254.49.125/16  node1         e0b     true
            node2_clus1  up/up    169.254.47.194/16  node2         e0a     true
            node2_clus2  up/up    169.254.19.183/16  node2         e0b     true

4 entries were displayed.


cluster1::*> *network device-discovery show -protocol cdp*

Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node2      /cdp
            e0a    cs1                       0/2               N9K-C92300YC
            e0b    cs2                       0/2               N9K-C92300YC
node1      /cdp
            e0a    cs1                       0/1               N9K-C92300YC
            e0b    cs2                       0/1               N9K-C92300YC

4 entries were displayed.


cs1# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID          Local Intrfce  Hldtme Capability  Platform      Port ID
node1               Eth1/1         124    H         FAS2750            e0a
node2               Eth1/2         124    H         FAS2750            e0a
cs2(FDO220329V5)    Eth1/65        179    R S I s   N9K-C92300YC  Eth1/65
cs2(FDO220329V5)    Eth1/66        179    R S I s   N9K-C92300YC  Eth1/66


cs2# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID          Local Intrfce  Hldtme Capability  Platform      Port ID
node1               Eth1/1         124    H         FAS2750            e0b
node2               Eth1/2         124    H         FAS2750            e0b
cs1(FDO220329KU)
                    Eth1/65        179    R S I s   N9K-C92300YC  Eth1/65
cs1(FDO220329KU)
                    Eth1/66        179    R S I s   N9K-C92300YC  Eth1/66

Total entries displayed: 4
----
====
. Verificare la connettività delle interfacce del cluster remoto:


[role="tabbed-block"]
====
.ONTAP 9.9.1 e versioni successive
--
È possibile utilizzare `network interface check cluster-connectivity` per avviare un controllo di accessibilità per la connettività del cluster e visualizzare i dettagli:

`network interface check cluster-connectivity start` e. `network interface check cluster-connectivity show`

[listing, subs="+quotes"]
----
cluster1::*> *network interface check cluster-connectivity start*
----
*NOTA:* attendere alcuni secondi prima di eseguire il `show` comando per visualizzare i dettagli.

[listing, subs="+quotes"]
----
cluster1::*> *network interface check cluster-connectivity show*
                                  Source           Destination      Packet
Node   Date                       LIF              LIF              Loss
------ -------------------------- ---------------- ---------------- -----------
node1
       3/5/2022 19:21:18 -06:00   node1_clus2      node2-clus1      none
       3/5/2022 19:21:20 -06:00   node1_clus2      node2_clus2      none
node2
       3/5/2022 19:21:18 -06:00   node2_clus2      node1_clus1      none
       3/5/2022 19:21:20 -06:00   node2_clus2      node1_clus2      none
----
--
.Tutte le release di ONTAP
--
Per tutte le release di ONTAP, è possibile utilizzare anche `cluster ping-cluster -node <name>` comando per controllare la connettività:

`cluster ping-cluster -node <name>`

[listing, subs="+quotes"]
----
cluster1::*> *cluster ping-cluster -node local*
Host is node2
Getting addresses from network interface table...
Cluster node1_clus1 169.254.209.69 node1     e0a
Cluster node1_clus2 169.254.49.125 node1     e0b
Cluster node2_clus1 169.254.47.194 node2     e0a
Cluster node2_clus2 169.254.19.183 node2     e0b
Local = 169.254.47.194 169.254.19.183
Remote = 169.254.209.69 169.254.49.125
Cluster Vserver Id = 4294967293
Ping status:
....
Basic connectivity succeeds on 4 path(s)
Basic connectivity fails on 0 path(s)
................
Detected 9000 byte MTU on 4 path(s):
    Local 169.254.19.183 to Remote 169.254.209.69
    Local 169.254.19.183 to Remote 169.254.49.125
    Local 169.254.47.194 to Remote 169.254.209.69
    Local 169.254.47.194 to Remote 169.254.49.125
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
----
--
====
. [[step3]] se è stata soppressa la creazione automatica dei casi, riattivarla richiamando un messaggio AutoSupport:
+
`system node autosupport invoke -node * -type all -message MAINT=END`

+
.Mostra esempio
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *system node autosupport invoke -node * -type all -message MAINT=END*
----
====
. Modificare nuovamente il livello di privilegio in admin:
+
`set -privilege admin`



.Quali sono le prossime novità?
link:../switch-cshm/config-overview.html["Configurare il monitoraggio dello stato dello switch"].
